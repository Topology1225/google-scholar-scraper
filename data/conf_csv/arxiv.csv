conference,title,writer,year,citations,url,paper ID,snippet
arxiv,"Sentiment_analysis:_Detecting_valence,_emotions,_and_other_affectual_states_from_text",SM Mohammad,2016,247,https://www.sciencedirect.com/science/article/pii/B9780081005088000096,6508685286338487216,"Sentiment analysis is the task of automatically determining from text the attitude, emotion, or some other affectual state of the author. This chapter summarizes the diverse landscape of tasks and applications associated with sentiment analysis. We outline key challenges …"
arxiv,A_primer_in_bertology:_What_we_know_about_how_bert_works,"A Rogers, O Kovaleva, A Rumshisky",2021,111,https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00349,3385490103150709983,"Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge …"
arxiv,Spanbert:_Improving_pre-training_by_representing_and_predicting_spans,"M Joshi, D Chen, Y Liu, DS Weld, L Zettlemoyer…",2020,285,https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00300,14676555484631167909,"We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the …"
arxiv,Winogrande:_An_adversarial_winograd_schema_challenge_at_scale,"K Sakaguchi, R Le Bras, C Bhagavatula…",2020,91,https://ojs.aaai.org/index.php/AAAI/article/view/6399,1875989222215830843,"Abstract The Winograd Schema Challenge (WSC)(Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on …"
arxiv,Flaubert:_Unsupervised_language_model_pre-training_for_french,"H Le, L Vial, J Frej, V Segonne, M Coavoux…",2019,51,https://arxiv.org/abs/1912.05372,15138518675658617129,"Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous …"
arxiv,Transformers:_State-of-the-art_natural_language_processing,"T Wolf, J Chaumond, L Debut, V Sanh…",2020,47,https://www.aclweb.org/anthology/2020.emnlp-demos.6/,2577386398084523747,Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize …
arxiv,Leveraging_pre-trained_checkpoints_for_sequence_generation_tasks,"S Rothe, S Narayan, A Severyn",2020,46,https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00313,395817795281596927,"Unsupervised pre-training of large neural models has recently revolutionized Natural Language Processing. By warm-starting from the publicly released checkpoints, NLP practitioners have pushed the state-of-the-art on multiple benchmarks while saving …"
arxiv,wav2vec_2.0:_A_framework_for_self-supervised_learning_of_speech_representations,"A Baevski, H Zhou, A Mohamed, M Auli",2020,42,https://arxiv.org/abs/2006.11477,17012233978100358310,We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent …
arxiv,Tanda:_Transfer_and_adapt_pre-trained_transformer_models_for_answer_sentence_selection,"S Garg, T Vu, A Moschitti",2020,36,https://ojs.aaai.org/index.php/AAAI/article/view/6282,7306705043279451725,"We propose TandA, an effective technique for fine-tuning pre-trained Transformer models for natural language tasks. Specifically, we first transfer a pre-trained model into a model for a general task by fine-tuning it with a large and high-quality dataset. We then perform a …"
arxiv,Unicoder-VL:_A_Universal_Encoder_for_Vision_and_Language_by_Cross-Modal_Pre-Training.,"G Li, N Duan, Y Fang, M Gong, D Jiang, M Zhou",2020,95,,7789950959158388367,"Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention …"
arxiv,Big_bird:_Transformers_for_longer_sequences,"M Zaheer, G Guruganesh, A Dubey, J Ainslie…",2020,34,https://arxiv.org/abs/2007.14062,11654897857579035055,"We propose to pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model (PMLM). Given an input text with masked tokens, we rely …"
arxiv,Unilmv2:_Pseudo-masked_language_models_for_unified_language_model_pre-training,"H Bao, L Dong, F Wei, W Wang…",2020,33,http://proceedings.mlr.press/v119/bao20a.html,17252701423323416900,"In this paper, we survey the methods and concepts developed for the evaluation of dialogue systems. Evaluation, in and of itself, is a crucial part during the development process. Often, dialogue systems are evaluated by means of human evaluations and questionnaires …"
arxiv,Survey_on_evaluation_methods_for_dialogue_systems,"J Deriu, A Rodrigo, A Otegi, G Echegoyen…",2020,31,https://link.springer.com/article/10.1007/s10462-020-09866-x,15784416199641307079,"AI has achieved remarkable mastery over games such as Chess, Go, and Poker, and even Jeopardy, but the rich variety of standardized exams has remained a landmark challenge. Even in 2016, the best AI system achieved merely 59.3% on an 8th Grade science exam …"
arxiv,From'F'to'A'on_the_NY_Regents_Science_Exams:_An_Overview_of_the_Aristo_Project,"P Clark, O Etzioni, D Khashabi, T Khot…",2019,27,https://arxiv.org/abs/1909.01958,13522467789861122032,"Joint image-text embedding is the bedrock for most Vision-and-Language (V+ L) tasks, where multimodality inputs are simultaneously processed for joint visual and textual understanding. In this paper, we introduce UNITER, a UNiversal Image-TExt …"
arxiv,Unified_Vision-Language_Pre-Training_for_Image_Captioning_and_VQA.,"L Zhou, H Palangi, L Zhang, H Hu, JJ Corso, J Gao",2020,84,,13315977689015101870,"Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to …"
arxiv,Uniter:_Universal_image-text_representation_learning,"YC Chen, L Li, L Yu, A El Kholy, F Ahmed…",2020,24,https://link.springer.com/chapter/10.1007/978-3-030-58577-8_7,3224460637705754187,"Open-domain question answering (QA) involves many knowledge and reasoning challenges, but are successful QA models actually learning such knowledge when trained on benchmark QA tasks? We investigate this via several new diagnostic tasks probing …"
arxiv,Q-BERT:_Hessian_Based_Ultra_Low_Precision_Quantization_of_BERT.,"S Shen, Z Dong, J Ye, L Ma, Z Yao, A Gholami…",2020,75,,12994389486235501420,"Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people to explore a better solution. As an alternative, self-supervised learning attracts many …"
arxiv,K-BERT:_Enabling_Language_Representation_with_Knowledge_Graph.,"W Liu, P Zhou, Z Zhao, Z Wang, Q Ju, H Deng, P Wang",2020,71,,8387397906481896703,"Fine-tuning pretrained contextual word embedding models to supervised downstream tasks has become commonplace in natural language processing. This process, however, is often brittle: even with the same hyperparameter values, distinct random seeds can lead to …"
arxiv,PIQA:_Reasoning_about_Physical_Commonsense_in_Natural_Language.,"Y Bisk, R Zellers, R LeBras, J Gao, Y Choi",2020,38,,10110424163152713144,"Understanding temporal dynamics of COVID-19 symptoms could provide fine-grained resolution to guide clinical decision-making. Here, we use deep neural networks over an institution-wide platform for the augmented curation of clinical notes from 77,167 patients …"
arxiv,Realm:_Retrieval-augmented_language_model_pre-training,"K Guu, K Lee, Z Tung, P Pasupat…",2020,98,https://arxiv.org/abs/2002.08909,10545104437231716997,"Abstract Machine Reading Comprehension (MRC) for question answering (QA), which aims to answer a question given the relevant context passages, is an important way to test the ability of intelligence systems to understand human language. Multiple-Choice QA (MCQA) …"
arxiv,What_does_my_qa_model_know?_devising_controlled_probes_using_expert_knowledge,"K Richardson, A Sabharwal",2020,13,https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00331,4419642556833878880,"We present an overview of the third edition of the Check-That! Lab at CLEF 2020. The lab featured five tasks in Arabic and English, and here we focus on the three English tasks. Task 1 challenged the participants to predict which tweets from a stream of tweets about COVID …"
arxiv,Self-supervised_learning:_Generative_or_contrastive,"X Liu, F Zhang, Z Hou, Z Wang, L Mian, J Zhang…",2020,13,https://arxiv.org/abs/2006.08218,6622103122828253066,DARPA and Allen AI have proposed a collection of datasets to encourage research in Question Answering domains where (commonsense) knowledge is expected to play an important role. Recent language models such as BERT and GPT that have been pre-trained …
arxiv,"Fine-tuning_pretrained_language_models:_Weight_initializations,_data_orders,_and_early_stopping","J Dodge, G Ilharco, R Schwartz, A Farhadi…",2020,62,https://arxiv.org/abs/2002.06305,11772005516281275773,"Because of its streaming nature, recurrent neural network transducer (RNN-T) is a very promising end-to-end (E2E) model that may replace the popular hybrid model for automatic speech recognition. In this paper, we describe our recent development of RNN-T models …"
arxiv,Augmented_curation_of_clinical_notes_from_a_massive_EHR_system_reveals_symptoms_of_impending_COVID-19_diagnosis,"T Wagner, FNU Shweta, K Murugadoss, S Awasthi…",2020,12,https://elifesciences.org/articles/58227,7455833942447289688,"Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an …"
arxiv,Mmm:_Multi-stage_multi-task_learning_for_multi-choice_reading_comprehension,"D Jin, S Gao, JY Kao, T Chung…",2020,11,https://ojs.aaai.org/index.php/AAAI/article/view/6310,17196324162402073459,"Prior work in visual dialog has focused on training deep neural models on VisDial in isolation. Instead, we present an approach to leverage pretraining on related vision-language datasets before transferring to visual dialog. We adapt the recently proposed …"
arxiv,Overview_of_CheckThat!_2020_English:_Automatic_identification_and_verification_of_claims_in_social_media,"S Shaar, A Nikolov, N Babulkov, F Alam…",2020,11,http://www.dei.unipd.it/~ferro/CLEF-WN-Drafts/CLEF2020/paper_265.pdf,16326235794459057143,"We present Mockingjay as a new speech representation learning approach, where bidirectional Transformer encoders are pre-trained on a large amount of unlabeled speech. Previous speech representation methods learn through conditioning on past frames and …"
arxiv,How_Additional_Knowledge_can_Improve_Natural_Language_Commonsense_Question_Answering?,"A Mitra, P Banerjee, KK Pal, S Mishra…",2019,11,https://www.researchgate.net/profile/Kuntal_Pal8/publication/335926645_Exploring_ways_to_incorporate_additional_knowledge_to_improve_Natural_Language_Commonsense_Question_Answering/links/5e52eedc458515072db79650/Exploring-ways-to-incorporate-additional-knowledge-to-improve-Natural-Language-Commonsense-Question-Answering.pdf,2041299835529470521,"One year ago, in the SIGIR Forum issue of December 2018, I ranted about the “neural hype”[9]. One year later, I write again to publicly recant my heretical beliefs. What a difference a year makes! In accelerated “deep learning” time, a year seems like an eternity …"
arxiv,Developing_RNN-T_models_surpassing_high-performance_hybrid_models_with_customization_capability,"J Li, R Zhao, Z Meng, Y Liu, W Wei…",2020,11,https://arxiv.org/abs/2007.15188,7713804886108936878,"Speech recognition technologies are gaining enormous popularity in various industrial applications. However, building a good speech recognition system usually requires significant amounts of transcribed data which is expensive to collect. To tackle this problem …"
arxiv,Ccnet:_Extracting_high_quality_monolingual_datasets_from_web_crawl_data,"G Wenzek, MA Lachaux, A Conneau…",2019,35,https://arxiv.org/abs/1911.00359,1579261859816674640,"We study the problem of injecting knowledge into large pre-trained models like BERT and RoBERTa. Existing methods typically update the original parameters of pre-trained models when injecting knowledge. However, when multiple kinds of knowledge are injected, they …"
arxiv,Large-scale_pretraining_for_visual_dialog:_A_simple_state-of-the-art_baseline,"V Murahari, D Batra, D Parikh, A Das",2020,10,https://link.springer.com/chapter/10.1007/978-3-030-58523-5_20,17504747839731273664,"Most syntactic dependency parsing models may fall into one of two categories: transition-and graph-based models. The former models enjoy high inference efficiency with linear time complexity, but they rely on the stacking or re-ranking of partially-built parse trees to build a …"
arxiv,Mockingjay:_Unsupervised_speech_representation_learning_with_deep_bidirectional_transformer_encoders,"AT Liu, S Yang, PH Chi, P Hsu…",2020,35,https://ieeexplore.ieee.org/abstract/document/9054458/,4466401828749761520,"We study the open-domain named entity recognition (NER) problem under distant supervision. The distant supervision, though does not require large amounts of manual annotations, yields highly incomplete and noisy distant labels via external knowledge bases …"
arxiv,Graph-Based_Reasoning_over_Heterogeneous_External_Knowledge_for_Commonsense_Question_Answering.,"S Lv, D Guo, J Xu, D Tang, N Duan, M Gong, L Shou…",2020,31,,10689307869405657692,"The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query. Although the most common formulation of text ranking is search, instances of the task can also be found in many natural language processing applications …"
arxiv,"Select,_Answer_and_Explain:_Interpretable_Multi-Hop_Reading_Comprehension_over_Multiple_Documents.","M Tu, K Huang, G Wang, J Huang, X He, B Zhou",2020,22,,13232992046333758831,"This paper describes the fourth edition of Lifelog challenges in ImageCLEF 2020. In this edition, the Lifelog challenges consist of two tasks which are Lifelog Moments Retrieval (LMRT) and Sport Performance Lifelog (SPLL). While the Lifelog Moments Retrieval …"
arxiv,"The_neural_hype,_justified!_A_recantation",J Lin,2019,7,https://cs.uwaterloo.ca/~jimmylin/publications/Lin_SIGIRForum2019.pdf,6669494556023494689,"Advanced neural language models (NLMs) are widely used in sequence generation tasks because they are able to produce fluent and meaningful sentences. They can also be used to generate fake reviews, which can then be used to attack online review systems and …"
arxiv,Improving_transformer-based_speech_recognition_using_unsupervised_pre-training,"D Jiang, X Lei, W Li, N Luo, Y Hu, W Zou…",2019,22,https://arxiv.org/abs/1910.09932,5772450302903437141,"One desired capability for machines is the ability to transfer their knowledge of one domain to another where data is (usually) scarce. Despite ample adaptation of transfer learning in various deep learning applications, we yet do not understand what enables a successful …"
arxiv,K-adapter:_Infusing_knowledge_into_pre-trained_models_with_adapters,"R Wang, D Tang, N Duan, Z Wei, X Huang…",2020,34,https://arxiv.org/abs/2002.01808,4166695802214878222,"Figurative language (FL) seems ubiquitous in all social media discussion forums and chats, posing extra challenges to sentiment analysis endeavors. Identification of FL schemas in short texts remains largely an unresolved issue in the broader field of natural language …"
arxiv,Global_greedy_dependency_parsing,"Z Li, H Zhao, K Parnow",2020,7,https://ojs.aaai.org/index.php/AAAI/article/view/6348,15024413624016070047,"Recently, multilingual question answering became a crucial research topic, and it is receiving increased interest in the NLP community. However, the unavailability of large-scale datasets makes it challenging to train multilingual QA systems with performance …"
arxiv,Knowledge_Distillation_from_Internal_Representations.,"G Aguilar, Y Ling, Y Zhang, B Yao, X Fan, C Guo",2020,22,,5508973971749999782,"The Arabic language is a morphologically rich and complex language with relatively little resources and a less explored syntax compared to English. Given these limitations, tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering …"
arxiv,Bond:_Bert-assisted_open-domain_named_entity_recognition_with_distant_supervision,"C Liang, Y Yu, H Jiang, S Er, R Wang, T Zhao…",2020,7,https://dl.acm.org/doi/abs/10.1145/3394486.3403149,9374103883212594888,"Sentiment analysis has been a hot research topic in natural language processing and data mining fields in the last decade. Recently, deep neural network (DNN) models are being applied to sentiment analysis tasks to obtain promising results. Among various neural …"
arxiv,Pretrained_transformers_for_text_ranking:_Bert_and_beyond,"J Lin, R Nogueira, A Yates",2020,6,https://arxiv.org/abs/2010.06467,8110050826903240127,"Good data stewardship requires removal of data at the request of the data's owner. This raises the question if and how a trained machine-learning model, which implicitly stores information about its training data, should be affected by such a removal request. Is it …"
arxiv,Overview_of_ImageCLEF_lifelog_2020:_lifelog_moment_retrieval_and_sport_performance_lifelog,"VT Ninh, TK Le, L Zhou, L Piras, M Riegler…",2020,6,http://ceur-ws.org/Vol-2696/paper_65.pdf,3446160353560662295,This paper presents the results of the wet lab information extraction task at WNUT 2020. This task consisted of two sub tasks:(1) a Named Entity Recognition (NER) task with 13 participants and (2) a Relation Extraction (RE) task with 2 participants. We outline the task …
arxiv,Generating_sentiment-preserving_fake_online_reviews_using_neural_language_models_and_their_human-and_machine-based_detection,"DI Adelani, H Mai, F Fang, HH Nguyen…",2020,18,https://link.springer.com/chapter/10.1007/978-3-030-44041-1_114,5676546788136507493,"The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring …"
arxiv,Evaluating_Commonsense_in_Pre-Trained_Language_Models.,"X Zhou, Y Zhang, L Cui, D Huang",2020,24,,8260365082044917165,"Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets, large computing systems, and better neural network models, natural language processing (NLP) technology has made significant strides in …"
arxiv,What_is_being_transferred_in_transfer_learning?,"B Neyshabur, H Sedghi, C Zhang",2020,6,https://arxiv.org/abs/2008.11687,13447249673581194617,"Extracting behavioral measurements non-invasively from video is stymied by the fact that it is a hard computational problem. Recent advances in deep learning have tremendously advanced our ability to predict posture directly from videos, which has quickly impacted …"
arxiv,A_transformer-based_approach_to_irony_and_sarcasm_detection,"RA Potamias, G Siolas, AG Stafylopatis",2020,6,https://link.springer.com/article/10.1007/s00521-020-05102-3,16378161650582262380,"Building accurate language models that capture meaningful long-term dependencies is a core challenge in natural language processing. Towards this end, we present a calibration-based approach to measure long-term discrepancies between a generative sequence …"
arxiv,Automatic_spanish_translation_of_the_squad_dataset_for_multilingual_question_answering,"CP Carrino, MR Costa-jussà, JAR Fonollosa",2019,6,https://arxiv.org/abs/1912.05200,17741137021962815094,"Society and individuals are negatively influenced both politically and socially by the widespread increase of fake news either way generated by humans or machines. In the era of social networks, the quick rotation of news makes it challenging to evaluate its reliability …"
arxiv,AraBERT:_Transformer-based_model_for_Arabic_language_understanding,"W Antoun, F Baly, H Hajj",2020,55,https://arxiv.org/abs/2003.00104,11773193403899396543,"Disruptions resulting from an epidemic might often appear to amount to chaos but, in reality, can be understood in a systematic way through the lens of"" epidemic psychology"". According to the father of this research field, Philip Strong, not only is the epidemic …"
arxiv,ABCDM:_An_attention-based_bidirectional_CNN-RNN_deep_model_for_sentiment_analysis,"ME Basiri, S Nemati, M Abdar, E Cambria…",2020,5,https://www.sciencedirect.com/science/article/pii/S0167739X20309195,943164472420871734,"Attempts to render deep learning models interpretable, data-efficient, and robust have seen some success through hybridisation with rule-based systems, for example, in Neural Theorem Provers (NTPs). These neuro-symbolic models can induce interpretable rules and …"
arxiv,Certified_data_removal_from_machine_learning_models,"C Guo, T Goldstein, A Hannun…",2019,19,https://arxiv.org/abs/1911.03030,5421394926787368463,"There has been significant progress in recent years in the field of Natural Language Processing thanks to the introduction of the Transformer architecture. Current state-of-the-art models, via a large number of parameters and pre-training on massive text corpus, have …"
arxiv,WNUT-2020_Task_1_Overview:_Extracting_Entities_and_Relations_from_Wet_Lab_Protocols,"J Tabassum, S Lee, W Xu, A Ritter",2020,5,https://arxiv.org/abs/2010.14576,7506943729088417833,"We study fact-checking in this paper, which aims to verify a textual claim given textual evidence (eg, retrieved sentences from Wikipedia). Existing studies typically either concatenate retrieved sentences as a single string or use feature fusion on the top of …"
arxiv,On_layer_normalization_in_the_transformer_architecture,"R Xiong, Y Yang, D He, K Zheng, S Zheng…",2020,28,https://arxiv.org/abs/2002.04745,13565805303124836944,This study examined records of 2566 consecutive COVID-19 patients at five Massachusetts hospitals and sought to predict level-of-care requirements based on clinical and laboratory data. Several classification methods were applied and compared against standard …
arxiv,SqueezeBERT:_What_can_computer_vision_teach_NLP_about_efficient_neural_networks?,"FN Iandola, AE Shaw, R Krishna…",2020,5,https://arxiv.org/abs/2006.11316,3109381338678448977,"Recent progress in NLP witnessed the development of large-scale pre-trained language models (GPT, BERT, XLNet, etc.) based on Transformer (Vaswani et al. 2017), and in a range of end tasks, such models have achieved state-of-the-art results, approaching human …"
arxiv,"A_primer_on_motion_capture_with_deep_learning:_principles,_pitfalls,_and_perspectives","A Mathis, S Schneider, J Lauer, MW Mathis",2020,5,https://www.sciencedirect.com/science/article/pii/S0896627320307170,17607914620493817448,"Fine-tuning pre-trained models have achieved impressive performance on standard natural language processing benchmarks. However, the resultant model generalizability remains poorly understood. We do not know, for example, how excellent performance can lead to the …"
arxiv,"Calibration,_entropy_rates,_and_memory_in_language_models","M Braverman, X Chen, S Kakade…",2020,5,http://proceedings.mlr.press/v119/braverman20a.html,5667657281050831176,"Existing literature on Question Answering (QA) mostly focuses on algorithmic novelty, data augmentation, or increasingly large pre-trained language models like XLNet and RoBERTa. Additionally, a lot of systems on the QA leaderboards do not have associated research …"
arxiv,Fake_news_stance_detection_using_deep_learning_architecture_(cnn-lstm),"M Umer, Z Imtiaz, S Ullah, A Mehmood, GS Choi…",2020,5,https://ieeexplore.ieee.org/abstract/document/9178321/,2518379563633186429,"Recent work has shown that pre-trained language models such as BERT improve robustness to spurious correlations in the dataset. Intrigued by these results, we find that the key to their success is generalization from a small amount of counterexamples where the …"
arxiv,How_epidemic_psychology_works_on_social_media:_evolution_of_responses_to_the_covid-19_pandemic,"LM Aiello, D Quercia, K Zhou, M Constantinides…",2020,4,https://arxiv.org/abs/2007.13169,11240394885296323624,"Abstract< p> Transformers have been proven a successful model for a variety of tasks in sequence modeling. However, computing the attention matrix, which is their key component, has quadratic complexity with respect to the sequence length, thus making them …"
arxiv,Learning_reasoning_strategies_in_end-to-end_differentiable_proving,"P Minervini, S Riedel, P Stenetorp…",2020,4,http://proceedings.mlr.press/v119/minervini20a.html,16334802341623350418,"We present our view of what is necessary to build an engaging open-domain conversational agent: covering the qualities of such an agent, the pieces of the puzzle that have been built so far, and the gaping holes we have not filled yet. We present a biased view, focusing on …"
arxiv,Stress_Test_Evaluation_of_Transformer-based_Models_in_Natural_Language_Understanding_Tasks,"C Aspillaga, A Carvallo, V Araujo",2020,4,https://arxiv.org/abs/2002.06261,2343933790884862325,"Motivation Inferring the properties of a protein from its amino acid sequence is one of the key problems in bioinformatics. Most state-of-the-art approaches for protein classification are tailored to single classification tasks and rely on handcrafted features, such as position …"
arxiv,Reasoning_over_semantic-level_graph_for_fact_checking,"W Zhong, J Xu, D Tang, Z Xu, N Duan, M Zhou…",2019,20,https://arxiv.org/abs/1909.03745,9203459882756615821,"We aim to improve question answering (QA) by decomposing hard questions into easier sub-questions that existing QA systems can answer. Since collecting labeled decompositions is cumbersome, we propose an unsupervised approach to produce sub-questions …"
arxiv,Early_prediction_of_level-of-care_requirements_in_patients_with_COVID-19,"B Hao, S Sotudian, T Wang, T Xu, Y Hu, A Gaitanidis…",2020,4,https://elifesciences.org/articles/60519,9408960427392309246,"Deep neural networks can directly learn from chemical structures without extensive, user-driven selection of descriptors in order to predict molecular properties/activities with high reliability. But these approaches typically require large training sets to learn the endpoint …"
arxiv,Attending_to_entities_for_better_text_understanding,"P Cheng, K Erk",2020,4,https://ojs.aaai.org/index.php/AAAI/article/view/6254,3580992881837692486,"In the NLP community, recent years have seen a surge of research activities that address machines' ability to perform deep language understanding which goes beyond what is explicitly stated in text, rather relying on reasoning and knowledge of the world. Many …"
arxiv,Can_fine-tuning_pre-trained_models_lead_to_perfect_nlp?_a_study_of_the_generalizability_of_relation_extraction,"N Zhang, L Li, S Deng, H Yu, X Cheng…",2020,4,https://arxiv.org/abs/2009.06206,11712926824419211946,"In this paper, we present a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of the optimization …"
arxiv,Frustratingly_easy_natural_question_answering,"L Pan, R Chakravarti, A Ferritto, M Glass…",2019,6,https://arxiv.org/abs/1909.05286,5944555183597346118,"We present CodeBERT, a bimodal pre-trained model for programming language (PL) and nat-ural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language codesearch, code documentation …"
arxiv,An_empirical_study_on_robustness_to_spurious_correlations_using_pre-trained_language_models,"L Tu, G Lalwani, S Gella, H He",2020,4,https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00335,3837500333593447979,"Short (15–30 residue) chains of amino acids at the amino termini of expressed proteins known as signal peptides (SPs) specify secretion in living cells. We trained an attention-based neural network, the Transformer model, on data from all available organisms in Swiss …"
arxiv,Fast_transformers_with_clustered_attention,"A Vyas, A Katharopoulos…",2020,4,https://proceedings.neurips.cc/paper/2020/hash/f6a8dd1c954c8506aadc764cc32b895e-Abstract.html,12028542204791594532,"With the rising success of adversarial attacks on many NLP tasks, systems which actually operate in an adversarial scenario need to be reevaluated. For this purpose, we pose the following research question: How difficult is it to fool automatic short answer grading …"
arxiv,"Open-domain_conversational_agents:_Current_progress,_open_problems,_and_future_directions","S Roller, YL Boureau, J Weston, A Bordes…",2020,3,https://arxiv.org/abs/2006.12442,12383332055434560570,Recent research has achieved impressive results on understanding and improving source code by building up on machine-learning techniques developed for natural languages. A significant advancement in natural-language understanding has come with the development …
arxiv,UDSMProt:_universal_deep_sequence_models_for_protein_classification,"N Strodthoff, P Wagner, M Wenzel, W Samek",2020,12,https://academic.oup.com/bioinformatics/article-abstract/36/8/2401/5698270,4662415277864409948,"Learning meaningful and general representations from unannotated speech that are applicable to a wide range of tasks remains challenging. In this paper we propose to use autoregressive predictive coding (APC), a recently proposed self-supervised objective, as a …"
arxiv,Unsupervised_question_decomposition_for_question_answering,"E Perez, P Lewis, W Yih, K Cho, D Kiela",2020,13,https://arxiv.org/abs/2002.09758,7253051980851582269,"Massive digital data processing provides a wide range of opportunities and benefits, but at the cost of endangering personal data privacy. Anonymisation consists in removing or replacing sensitive information from data, enabling its exploitation for different purposes …"
arxiv,Inductive_transfer_learning_for_molecular_activity_prediction:_Next-Gen_QSAR_Models_with_MolPMoFiT,"X Li, D Fourches",2020,13,https://link.springer.com/content/pdf/10.1186/s13321-020-00430-x.pdf,17742656462062113812,"Our goal of patent claim generation is to realize"" augmented inventing"" for inventors by leveraging latest Deep Learning techniques. We envision the possibility of building an"" auto-complete"" function for inventors to conceive better inventions in the era of artificial …"
arxiv,"Recent_advances_in_natural_language_inference:_A_survey_of_benchmarks,_resources,_and_approaches","S Storks, Q Gao, JY Chai",2019,8,https://arxiv.org/abs/1904.01172,9020144390452081564,"Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer …"
arxiv,Prophetnet:_Predicting_future_n-gram_for_sequence-to-sequence_pre-training,"Y Yan, W Qi, Y Gong, D Liu, N Duan, J Chen…",2020,34,https://arxiv.org/abs/2001.04063,15089546621535901138,"Humor is a complicated language phenomenon that depends upon many factors, including topic, date, and recipient. Because of this variation, it can be hard to determine what exactly makes a joke humorous, leading to difficulties in joke identification and related tasks …"
arxiv,Codebert:_A_pre-trained_model_for_programming_and_natural_languages,"Z Feng, D Guo, D Tang, N Duan, X Feng…",2020,30,https://arxiv.org/abs/2002.08155,9055786889913621082,"Humans carry stereotypic tacit assumptions (STAs)(Prince, 1978), or propositional beliefs about generic concepts. Such associations are crucial for understanding natural language. We construct a diagnostic set of word prediction prompts to evaluate whether recent neural …"
arxiv,Signal_peptides_generated_by_attention-based_neural_networks,"Z Wu, KK Yang, MJ Liszka, A Lee, A Batzilla…",2020,3,https://pubs.acs.org/doi/abs/10.1021/acssynbio.0c00219,17286751942397368761,"Many recent studies have shown that for models trained on datasets for natural language inference (NLI), it is possible to make correct predictions by merely looking at the hypothesis while completely ignoring the premise. In this work, we manage to derive adversarial …"
arxiv,Fooling_automatic_short_answer_grading_systems,"A Filighera, T Steuer, C Rensing",2020,3,https://link.springer.com/chapter/10.1007/978-3-030-52237-7_15,3258282559731759376,We propose a self-supervised representation learning model for the task of unsupervised phoneme boundary detection. The model is a convolutional neural network that operates directly on the raw waveform. It is optimized to identify spectral changes in the signal using …
arxiv,Learning_and_Evaluating_Contextual_Embedding_of_Source_Code,"A Kanade, P Maniatis…",2020,3,http://proceedings.mlr.press/v119/kanade20a.html,6167756387594879945,"Identifying a divergence problem in Adam, we propose a new optimizer, LaProp, which belongs to the family of adaptive gradient descent methods. This method allows for greater flexibility in choosing its hyperparameters, mitigates the effort of fine tuning, and permits …"
arxiv,Generative_pre-training_for_speech_with_autoregressive_predictive_coding,"YA Chung, J Glass",2020,23,https://ieeexplore.ieee.org/abstract/document/9054438/,3706484636669618577,"Named entity recognition (NER) is key for biomedical applications as it allows knowledge discovery in free text data. As entities are semantic phrases, their meaning is conditioned to the context to avoid ambiguity. In this work, we explore contextualized language models for …"
arxiv,Sensitive_data_detection_and_classification_in_spanish_clinical_text:_Experiments_with_bert,"A García-Pablos, N Perez, M Cuadros",2020,3,https://arxiv.org/abs/2003.03106,1537149522243131495,"Natural language processing (NLP) and neural networks (NNs) have both undergone significant changes in recent years. For active learning (AL) purposes, NNs are, however, less commonly used--despite their current popularity. By using the superior text classification …"
arxiv,Measuring_Patent_Claim_Generation_by_Span_Relevancy,"JS Lee, J Hsiang",2019,5,https://arxiv.org/abs/1908.09591,11500800430209529081,"On the NLP4IF 2019 sentence level propaganda classification task, we used a BERT language model that was pre-trained on Wikipedia and BookCorpus as team ltuorp ranking\# 1 of 26. It uses deep learning in the form of an attention transformer. We …"
arxiv,Infusing_Knowledge_into_the_Textual_Entailment_Task_Using_Graph_Convolutional_Networks.,"P Kapanipathi, V Thost, SS Patel, S Whitehead…",2020,5,,16298494684052071876,"SINCE 2012, THE field of artificial intelligence (AI) has reported remarkable progress on a broad 
range of capabilities including object recognition, game playing, speech recognition, and machine 
translation.43 Much of this progress has been achieved by increasingly large and computationally …"
arxiv,Training_question_answering_models_from_synthetic_data,"R Puri, R Spring, M Patwary, M Shoeybi…",2020,12,https://arxiv.org/abs/2002.09599,10851712546194088064,"Recently, the pandemic of the novel Coronavirus Disease-2019 (COVID-19) has presented governments with ultimate challenges. In the United States, the country with the highest confirmed COVID-19 infection cases, a nationwide social distancing protocol has been …"
arxiv,The_rjokes_dataset:_a_large_scale_humor_collection,"O Weller, K Seppi",2020,3,https://www.aclweb.org/anthology/2020.lrec-1.753/,13862602728456782088,"We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments …"
arxiv,Probing_neural_language_models_for_human_tacit_assumptions,"N Weir, A Poliak, B Van Durme - 2020 - cogsci.mindmodeling.org",2020,2,https://cogsci.mindmodeling.org/2020/papers/0070/0070.pdf,8046339947552928978,"Word embedding, which represents individual words with semantically fixed-length vectors, has made it possible to successfully apply deep learning to natural language processing tasks such as semantic role-modeling, question answering, and machine translation. As …"
arxiv,HypoNLI:_Exploring_the_artificial_patterns_of_hypothesis-only_bias_in_natural_language_inference,"T Liu, X Zheng, B Chang, Z Sui",2020,2,https://arxiv.org/abs/2003.02756,5922186282747029386,"Neural ranking models are traditionally trained on a series of random batches, sampled uniformly from the entire training set. Curriculum learning has recently been shown to improve neural models' effectiveness by sampling batches non-uniformly, going from easy to …"
arxiv,Self-supervised_contrastive_learning_for_unsupervised_phoneme_segmentation,"F Kreuk, J Keshet, Y Adi",2020,2,https://arxiv.org/abs/2007.13465,1796509065840248643,"The medical literature has been growing exponentially, and its size has become a barrier for physicians to locate and extract clinically useful information. As a promising solution, natural language processing (NLP), especially machine learning (ML)‐based NLP is a technology …"
arxiv,Laprop:_a_better_way_to_combine_momentum_with_adaptive_gradient,"L Ziyin, ZT Wang, M Ueda",2020,7,https://arxiv.org/abs/2002.04839,8052306576648300694,"We present our team '3Idiots'(referred as 'sdhanshu'in the official rankings) approach for the Trolling, Aggression and Cyberbullying (TRAC) 2020 shared tasks. Our approach relies on fine-tuning various Transformer models on the different datasets. We also investigated the …"
arxiv,Contextualized_French_language_models_for_biomedical_named_entity_recognition,"J Copara, J Knafou, N Naderi, C Moro, P Ruch…",2020,2,https://www.aclweb.org/anthology/2020.jeptalnrecital-deft.4/,10346008598124814969,"One of the obstacles of abstractive summarization is the presence of various potentially correct predictions. Widely used objective functions for supervised learning, such as cross-entropy loss, cannot handle alternative answers effectively. Rather, they act as a training …"
arxiv,A_Survey_of_Active_Learning_for_Text_Classification_using_Deep_Neural_Networks,"C Schröder, A Niekler",2020,2,https://arxiv.org/abs/2008.07267,6047757928505622327,"Recently, pre-trained language representation flourishes as the mainstay of the natural language understanding community, eg, BERT. These pre-trained language representations can create state-of-the-art results on a wide range of downstream tasks. Along with …"
arxiv,Divisive_language_and_propaganda_detection_using_multi-head_attention_transformers_with_deep_learning_BERT-based_language_models_for_binary_classification,"N Mapes, A White, R Medury, S Dua",2019,7,https://www.aclweb.org/anthology/D19-5014/,11797111986931173179,Abstract Machine Reading Comprehension (MRC) is a challenging Natural Language Processing (NLP) research field with wide real-world applications. The great progress of this field in recent years is mainly due to the emergence of large-scale datasets and deep …
arxiv,Green_AI,"R Schwartz, J Dodge, NA Smith, O Etzioni",2020,2,https://dl.acm.org/doi/abs/10.1145/3381831,16236207167150263254,"Pivot-based neural representation models have led to significant progress in domain adaptation for NLP. However, previous research following this approach utilize only labeled data from the source domain and unlabeled data from the source and target domains, but …"
arxiv,The_ivory_tower_lost:_How_college_students_respond_differently_than_the_general_public_to_the_covid-19_pandemic,"V Duong, P Pham, T Yang, Y Wang, J Luo",2020,11,https://arxiv.org/abs/2004.09968,10662626771483211547,"Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this work, we provide …"
